{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a0160-361b-466a-a20a-6f03e9770e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from seleniumbase import Driver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#Nawy URL\n",
    "START_URL = \"https://www.nawy.com/compound/939-telal-east\"\n",
    "\n",
    "def get_compound_links(driver):\n",
    "    print(\"ðŸ” Ø¬Ø§Ø±ÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ (Top Compounds)...\")\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 0.7);\")\n",
    "    time.sleep(3)\n",
    "    links = [el.get_attribute(\"href\") for el in driver.find_elements(By.XPATH, \"//a[contains(@href, '/compound/')]\")]\n",
    "    unique = [l for l in list(dict.fromkeys(links)) if not any(x in l.lower() for x in ['sale', 'unit', 'property'])]\n",
    "    return unique\n",
    "\n",
    "def get_unit_links_selenium(driver, compound_url):\n",
    "    \"\"\"Ø§Ø³ØªØ®Ø¯Ø§Ù… Selenium ÙÙ‚Ø· Ù„Ù„ØªÙ†Ù‚Ù„ Ø¨ÙŠÙ† Ø§Ù„ØµÙØ­Ø§Øª ÙˆØ¬Ù…Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·\"\"\"\n",
    "    driver.get(compound_url)\n",
    "    time.sleep(5)\n",
    "    all_unit_links = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        print(f\"  ðŸ“„ Ø¬Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙØ­Ø§Øª Ù„Ù€ {compound_url.split('/')[-1]} [ØµÙØ­Ø© {page}]\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        unit_els = driver.find_elements(By.CSS_SELECTOR, \"a[href*='/property/']\")\n",
    "        all_unit_links.extend([el.get_attribute(\"href\") for el in unit_els])\n",
    "        \n",
    "        try:\n",
    "            next_btn = driver.find_element(By.XPATH, \"//li[contains(@class, 'next')]//a\")\n",
    "            if \"disabled\" in next_btn.find_element(By.XPATH, \"..\").get_attribute(\"class\"): break\n",
    "            driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "            page += 1\n",
    "            time.sleep(3)\n",
    "        except: break\n",
    "    return list(dict.fromkeys(all_unit_links))\n",
    "\n",
    "def fast_parse_details(url):\n",
    "    \"\"\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… requests (Ø£Ø³Ø±Ø¹ Ø¨Ù€ 20 Ù…Ø±Ø© Ù…Ù† Selenium)\"\"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Search the container or card that include data we need\n",
    "        container = soup.select_one(\"div.propertyDetails\")\n",
    "        if not container: return None\n",
    "        \n",
    "        lines = [l.strip() for l in container.get_text(separator=\"\\n\").split('\\n') if l.strip()]\n",
    "        price_text = soup.select_one(\"div.price, [class*='price']\").get_text() if soup.select_one(\"div.price\") else \"0\"\n",
    "        price = re.sub(r'[^\\d.]', '', price_text)\n",
    "        data = {\n",
    "            \"URL\": url, \"Type\": lines[0] if len(lines) > 0 else \"N/A\",\n",
    "            \"Area\": lines[1] if len(lines) > 1 else \"N/A\", \"Price_EGP\": price,\n",
    "            \"Reference_No\": \"N/A\", \"Bedrooms\": \"N/A\", \"Bathrooms\": \"N/A\",\n",
    "            \"Delivery\": \"N/A\", \"Compound\": \"N/A\", \"Finishing\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        mapping = {\"Reference No.\": \"Reference_No\", \"Bedrooms\": \"Bedrooms\", \"Bathrooms\": \"Bathrooms\", \n",
    "                   \"Delivery In\": \"Delivery\", \"Compound\": \"Compound\", \"Finishing\": \"Finishing\"}\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            for key, col in mapping.items():\n",
    "                if key in line and i+1 < len(lines): data[col] = lines[i+1]\n",
    "        return data\n",
    "    except: return None\n",
    "\n",
    "# after exploring pages via selenium and parsing data via beatiflsoup it is driver role to extract the data rapidly \n",
    "driver = Driver(uc=True, headless=True) \n",
    "try:\n",
    "    driver.get(START_URL)\n",
    "    compounds = get_compound_links(driver)\n",
    "    \n",
    "    all_links_to_scrape = []\n",
    "    for cp in compounds:\n",
    "        all_links_to_scrape.extend(get_unit_links_selenium(driver, cp))\n",
    "    \n",
    "    print(f"\nðŸš€ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(all_links_to_scrape)}")\n",
    "    print(\"â³ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© Ø¨Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ©...\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        final_results = list(executor.map(fast_parse_details, all_links_to_scrape))\n",
    "\n",
    "    final_results = [r for r in final_results if r]\n",
    "    df = pd.DataFrame(final_results)\n",
    "    df.to_csv(\"Nawy_scraping.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f"ðŸŽ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡! ØªÙ… Ø­ÙØ¸ {len(df)} ÙˆØ­Ø¯Ø© Ø¨Ø¯Ù‚Ø© ÙƒØ§Ù…Ù„Ø©.")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
